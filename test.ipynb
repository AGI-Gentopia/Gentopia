{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-13T00:30:51.937737272Z",
     "start_time": "2023-06-13T00:30:51.454082421Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from gentopia.prompt.rewoo import ZeroShotPlannerPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI agent who makes step-by-step plans to solve a problem under the help of external tools. \n",
      "For each step, make one plan followed by one tool-call, which will be executed later to retrieve evidence for that step.\n",
      "You should store each evidence into a distinct variable #E1, #E2, #E3 ... that can be referred to in later tool-call inputs.    \n",
      "\n",
      "##Available Tools##\n",
      "Google\n",
      "\n",
      "##Output Format (Replace '<...>')##\n",
      "*Plan1: <describe your plan here>\n",
      "*E1: <ToolName>[<input>]\n",
      "*Plan2: <describe next plan>\n",
      "*E2: <ToolName>[<input, you can use #E1 to represent its expected output>]\n",
      "And so on...\n",
      "  \n",
      "##Your Task##\n",
      "what is 1\n",
      "\n",
      "##Now Begin##\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ZeroShotPlannerPrompt.format(tool_description=\"Google\", task=\"what is 1\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T00:30:51.939792661Z",
     "start_time": "2023-06-13T00:30:51.938404477Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"tool_description\", \"task\"],\n",
    "    template=ZeroShotPlannerPrompt,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T00:22:54.456118921Z",
     "start_time": "2023-06-13T00:22:54.455705703Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI agent who makes step-by-step plans to solve a problem under the help of external tools. \n",
      "For each step, make one plan followed by one tool-call, which will be executed later to retrieve evidence for that step.\n",
      "You should store each evidence into a distinct variable #E1, #E2, #E3 ... that can be referred to in later tool-call inputs.    \n",
      "\n",
      "##Available Tools##\n",
      "a hammer\n",
      "\n",
      "##Output Format (Replace '<...>')##\n",
      "*Plan1: <describe your plan here>\n",
      "*E1: <ToolName>[<input>]\n",
      "*Plan2: <describe next plan>\n",
      "*E2: <ToolName>[<input, you can use #E1 to represent its expected output>]\n",
      "And so on...\n",
      "  \n",
      "##Your Task##\n",
      "hammering nails\n",
      "\n",
      "##Now Begin##\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = prompt.format(tool_description=\"a hammer\", task=\"hammering nails\")\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T00:23:06.717886556Z",
     "start_time": "2023-06-13T00:23:06.717279167Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nI want you to act as a naming consultant for new companies.\\nWhat is a good name for a company that makes cars?\\n'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.format(product=\"cars\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T17:45:48.144749391Z",
     "start_time": "2023-06-11T17:45:48.141402093Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "100.0"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rapidfuzz import fuzz\n",
    "fuzz.partial_ratio(\"a hammer\", \"hammer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T00:35:57.001351089Z",
     "start_time": "2023-06-13T00:35:56.999597809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on CUDA\n"
     ]
    }
   ],
   "source": [
    "if next(model.parameters()).is_cuda:\n",
    "    print(\"Model is on CUDA\")\n",
    "else:\n",
    "    print(\"Model is on CPU\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T07:19:23.956972651Z",
     "start_time": "2023-06-16T07:19:23.951967470Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/billxbf/anaconda3/envs/Gentopia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('local/billxbf'), PosixPath('@/tmp/.ICE-unix/1747,unix/billxbf')}\n",
      "  warn(msg)\n",
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f65ccd135b0946be8816905de9aa3ee9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "adapters_name = 'timdettmers/guanaco-7b'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapters_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T07:19:19.306832703Z",
     "start_time": "2023-06-16T07:18:34.168774467Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant.The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "### Human: What is the best way to make a hammer? ### Assistant: The best way to make a hammer depends on the type of hammer you want to make and the materials you have available. Here are some general steps you can follow to make a hammer:\n",
      "\n",
      "1. Gather the materials: The first step is to gather the materials you need to make the hammer. You will need a handle, a head, and a shaft. The type of handle and head you use will depend on the type of hammer you want to make.\n",
      "\n",
      "2. Shape the handle: The handle is the part of the hammer that you will hold. You can use a saw, a drill, or a file to shape the handle to your desired size and shape.\n",
      "\n",
      "3. Attach the head: Once you have shaped the handle, you can attach the head to it. You can use a wrench or a hammer to bang the head into place.\n",
      "\n",
      "4. Sharpen the head: If you are making a hammer for driving nails, you will need to sharpen the head. You can do this by using a file or a grinder to sharpen the head.\n",
      "\n",
      "5. Attach the shaft: The shaft is the part of the hammer that connects the handle to the head. You can use a wrench or a hammer to attach the shaft to the head.\n",
      "\n",
      "6. Finish the hammer: Once you have attached the shaft and the head, you can finish the hammer by sanding it and applying a coat of paint or varnish.\n",
      "\n",
      "These are the general steps you can follow to make a hammer. The exact steps may vary depending on the type of hammer you are making and the materials you have available.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the best way to make a hammer?\"\n",
    "formatted_prompt = (\n",
    "    f\"A chat between a curious human and an artificial intelligence assistant.\"\n",
    "    f\"The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\"\n",
    "    f\"### Human: {prompt} ### Assistant:\"\n",
    ")\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "outputs = model.generate(inputs=inputs.input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T12:39:44.859204099Z",
     "start_time": "2023-06-15T12:39:34.430724301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "a = \"\"\n",
    "if a:\n",
    "    print(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T05:44:03.937256697Z",
     "start_time": "2023-06-16T05:44:03.935624576Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data =  json.load(open(\"gentopia/resource/model_cards.json\", encoding=\"utf-8\"))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T06:31:17.678365852Z",
     "start_time": "2023-06-16T06:31:17.630638650Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['t5-vicuna-3b', 'replit-3b', 'camel-5b', 'mpt-7b', 'redpajama-7b', 'redpajama-instruct-7b', 'alpaca-lora-7b', 'alpaca-lora-13b', 'koalpaca', 'kullm', 'stablelm-7b', 'os-stablelm-7b', 'stackllama-7b', 'flan-3b', 'baize-7b', 'gpt4-alpaca-7b', 'vicuna-7b', 'baize-13b', 'vicuna-13b', 'gpt4-alpaca-13b', 'flan-11b', 'stable-vicuna-13b', 'camel-20b', 'starchat-15b', 'starchat-beta-15b', 'llama-deus-7b', 'evolinstruct-vicuna-7b', 'evolinstruct-vicuna-13b', 'alpacoom-7b', 'guanaco-7b', 'guanaco-13b', 'guanaco-33b', 'falcon-7b', 'falcon-40b', 'wizard-falcon-7b', 'wizard-falcon-40b', 'nous-hermes-13b', 'oa-llama-30b', 'airoboros-7b', 'airoboros-13b', 'samantha-7b', 'samantha-13b', 'samantha-33b', 'lazarus-30b', 'chronos-13b', 'chronos-33b', 'wizardlm-13b', 'wizardlm-30b', 'wizard-vicuna-13b', 'wizard-vicuna-30b'])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T02:56:56.411162577Z",
     "start_time": "2023-06-16T02:56:56.409926356Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'category': '<10B',\n 'display_name': 'Flan-XL',\n 'thumb': 'https://i.ibb.co/yBTk5bv/flan.png',\n 'parameters': '3 Billion',\n 'hub(base)': 'declare-lab/flan-alpaca-xl',\n 'hub(ckpt)': 'N/A',\n 'default_gen_config': 'configs/response_configs/flan.yaml',\n 'desc': 'flan-alpaca-xl is [Flan-T5-XL](https://huggingface.co/google/flan-t5-xl) 3B fine-tuned on [Flan](https://github.com/google-research/FLAN) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) datasets.',\n 'example1': [['Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.',\n   '']],\n 'example2': [['Can you help me write a resignation letter to my current employer, while leaving on good terms and expressing gratitude for the opportunities provided?',\n   '']],\n 'example3': [['What factors would you consider when designing an inclusive and accessible public transportation system?',\n   '']],\n 'example4': [['Write a python program to print the first 10 Fibonacci numbers',\n   '']]}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"flan-3b\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T06:31:34.456033445Z",
     "start_time": "2023-06-16T06:31:34.446648276Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from gentopia.llm.client.huggingface import HuggingfaceLLMClient\n",
    "from gentopia.model.param_model import HuggingfaceParamModel\n",
    "\n",
    "param = HuggingfaceParamModel(\n",
    "    model_name=\"guanaco-7b\"\n",
    ")\n",
    "\n",
    "client = HuggingfaceLLMClient(model_name=\"guanaco-7b\", model_param=param, device=\"gpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T08:14:19.709690171Z",
     "start_time": "2023-06-16T08:14:19.182163753Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/billxbf/anaconda3/envs/Gentopia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('local/billxbf'), PosixPath('@/tmp/.ICE-unix/1747,unix/billxbf')}\n",
      "  warn(msg)\n",
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/billxbf/anaconda3/envs/Gentopia/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c67925c59f2f4956bb835938c47ba39d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client.completion(\"what is 1+2\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-16T08:14:20.797734292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
